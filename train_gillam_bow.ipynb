{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc8cc7d2-12bb-4256-b5b6-619bffd8344c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/dev/test splits ...\n",
      "Train subjects: 540, Dev: 68, Test: 68\n",
      "Vocab size: 3700\n",
      "Input size: 3700, Hidden size: 100, Output size: 2\n",
      "[Epoch 01] loss=0.5827, dev_acc=0.7500\n",
      "[Epoch 02] loss=0.5550, dev_acc=0.7500\n",
      "[Epoch 03] loss=0.4818, dev_acc=0.7500\n",
      "[Epoch 04] loss=0.4826, dev_acc=0.8088\n",
      "[Epoch 05] loss=0.4789, dev_acc=0.8235\n",
      "[Epoch 06] loss=0.4488, dev_acc=0.6618\n",
      "[Epoch 07] loss=0.4757, dev_acc=0.7353\n",
      "[Epoch 08] loss=0.4411, dev_acc=0.7353\n",
      "[Epoch 09] loss=0.4226, dev_acc=0.7941\n",
      "[Epoch 10] loss=0.4288, dev_acc=0.7941\n",
      "[Epoch 11] loss=0.4231, dev_acc=0.7941\n",
      "[Epoch 12] loss=0.4016, dev_acc=0.8382\n",
      "[Epoch 13] loss=0.4092, dev_acc=0.7647\n",
      "[Epoch 14] loss=0.4214, dev_acc=0.7500\n",
      "[Epoch 15] loss=0.4005, dev_acc=0.7500\n",
      "[Epoch 16] loss=0.3929, dev_acc=0.7941\n",
      "[Epoch 17] loss=0.3716, dev_acc=0.8088\n",
      "[Epoch 18] loss=0.3525, dev_acc=0.8088\n",
      "[Epoch 19] loss=0.3975, dev_acc=0.7353\n",
      "[Epoch 20] loss=0.3629, dev_acc=0.8382\n",
      "[Epoch 21] loss=0.3489, dev_acc=0.8235\n",
      "[Epoch 22] loss=0.3828, dev_acc=0.8235\n",
      "[Epoch 23] loss=0.3707, dev_acc=0.7794\n",
      "[Epoch 24] loss=0.3712, dev_acc=0.8235\n",
      "[Epoch 25] loss=0.3939, dev_acc=0.8235\n",
      "[Epoch 26] loss=0.4209, dev_acc=0.8529\n",
      "[Epoch 27] loss=0.3834, dev_acc=0.7353\n",
      "[Epoch 28] loss=0.3317, dev_acc=0.8382\n",
      "[Epoch 29] loss=0.3398, dev_acc=0.7794\n",
      "[Epoch 30] loss=0.3598, dev_acc=0.7941\n",
      "\n",
      "[TEST] accuracy = 0.7206\n",
      "\n",
      "Confusion Matrix (rows = true, cols = pred):\n",
      "          pred_SLI   pred_TD\n",
      "true_SLI        12         7\n",
      "true_TD         12        37\n",
      "\n",
      "Per-class metrics:\n",
      "Class   Precision   Recall   F1-score   Support\n",
      "SLI       0.5000   0.6316     0.5581        19\n",
      "TD        0.8409   0.7551     0.7957        49\n",
      "\n",
      "Macro-averaged:\n",
      "Precision: 0.6705, Recall: 0.6933, F1: 0.6769\n",
      "\n",
      "Overall Accuracy: 0.7206\n"
     ]
    }
   ],
   "source": [
    "# train_gillam_bow.py\n",
    "# baseline: Bag-of-Words + TwoLayerNet (from scratch, fully in one file)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import extract_utterances  # clean_text 포함된 Utterance 제공\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0. 순수 NumPy로 구현한 신경망 구성요소들 (layers, optimizer, TwoLayerNet)\n",
    "# ============================================================\n",
    "\n",
    "# --- basic functions ---\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        y = np.exp(x)\n",
    "        y /= y.sum(axis=1, keepdims=True)\n",
    "    else:\n",
    "        x = x - np.max(x)\n",
    "        y = np.exp(x) / np.sum(np.exp(x))\n",
    "    return y\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    \"\"\"y: softmax 출력, t: one-hot 또는 정수 라벨\"\"\"\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(1, -1)\n",
    "        t = t.reshape(1, -1)\n",
    "\n",
    "    # one-hot이면 정수 라벨로 변환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    # 작은 값 더해서 log(0) 방지\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "# --- layers ---\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        self.x = x\n",
    "        out = x.dot(W) + b\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = dout.dot(W.T)\n",
    "        dW = self.x.T.dot(dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.y = None  # softmax 결과\n",
    "        self.t = None  # 정답 레이블(one-hot 또는 정수)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        # t가 one-hot인 경우\n",
    "        if self.t.size == self.y.size:\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx /= batch_size\n",
    "\n",
    "        return dx * dout\n",
    "\n",
    "\n",
    "# --- optimizer ---\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for p, g in zip(params, grads):\n",
    "            p -= self.lr * g\n",
    "\n",
    "\n",
    "# --- TwoLayerNet ---\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \"\"\"\n",
    "    입력층 - 은닉층 - 출력층 2층 신경망 (완전 수제 구현)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "\n",
    "        # 가중치 초기화 (작은 값)\n",
    "        W1 = 0.01 * np.random.randn(I, H).astype(np.float32)\n",
    "        b1 = np.zeros(H, dtype=np.float32)\n",
    "        W2 = 0.01 * np.random.randn(H, O).astype(np.float32)\n",
    "        b2 = np.zeros(O, dtype=np.float32)\n",
    "\n",
    "        # 레이어 구성\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # 모든 파라미터, 기울기 모으기\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        score = self.predict(x)\n",
    "        loss = self.loss_layer.forward(score, t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. CHA → 한 아이의 텍스트로 변환\n",
    "# ============================================================\n",
    "\n",
    "def load_child_text(cha_path: Path, speakers=('CHI',)) -> str:\n",
    "    \"\"\"\n",
    "    한 .cha 파일에서 지정 화자(speakers)의 clean_text를 전부 이어붙인 문자열 반환\n",
    "    \"\"\"\n",
    "    utts = extract_utterances(str(cha_path), list(speakers))\n",
    "    texts = [u.clean_text for u in utts if u.clean_text]\n",
    "\n",
    "    # 발화가 아예 없으면 빈 문자열\n",
    "    return \" \".join(texts)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. CSV에서 subjects 로딩\n",
    "# ============================================================\n",
    "\n",
    "def load_split_csv(csv_path: Path, base_dir: Path, speakers=('CHI',)):\n",
    "    \"\"\"\n",
    "    gillam_train/dev/test.csv 를 읽고\n",
    "    각 row(=subject)에 대해 CHI 발화를 모두 모은 텍스트를 생성\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    label_to_idx = {\"SLI\": 0, \"TD\": 1}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        cha_rel = row[\"filename\"]          # 예: 'gillam/SLI/5m/xxx.cha'\n",
    "        cha_path = (base_dir / cha_rel).resolve()\n",
    "\n",
    "        text = load_child_text(cha_path, speakers=speakers)\n",
    "\n",
    "        # 혹시라도 비어 있으면 스킵 (원하면 assert로 변경 가능)\n",
    "        if len(text.strip()) == 0:\n",
    "            print(f\"[WARN] Empty utterance: {cha_path}\")\n",
    "            continue\n",
    "\n",
    "        texts.append(text)\n",
    "        labels.append(label_to_idx[row[\"group\"]])\n",
    "\n",
    "    labels = np.array(labels, dtype=np.int64)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Vocab + Bag-of-Words\n",
    "# ============================================================\n",
    "\n",
    "def build_vocab(texts, min_freq=1, max_size=None):\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(t.lower().split())\n",
    "\n",
    "    # <unk> 토큰 포함\n",
    "    vocab = [\"<unk>\"]\n",
    "    for w, c in counter.most_common():\n",
    "        if c < min_freq:\n",
    "            continue\n",
    "        vocab.append(w)\n",
    "        if max_size is not None and len(vocab) >= max_size:\n",
    "            break\n",
    "\n",
    "    word_to_id = {w: i for i, w in enumerate(vocab)}\n",
    "    return vocab, word_to_id\n",
    "\n",
    "\n",
    "def text_to_bow(text, word_to_id):\n",
    "    vec = np.zeros(len(word_to_id), dtype=np.float32)\n",
    "    for w in text.lower().split():\n",
    "        idx = word_to_id.get(w, word_to_id[\"<unk>\"])\n",
    "        vec[idx] += 1.0\n",
    "    return vec\n",
    "\n",
    "\n",
    "def build_bow_matrix(texts, labels, word_to_id):\n",
    "    X = np.stack([text_to_bow(t, word_to_id) for t in texts])  # (N, V)\n",
    "    T = np.eye(2, dtype=np.float32)[labels]  # one-hot targets (SLI=0, TD=1)\n",
    "    return X, T\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. 학습 & 평가 유틸\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_accuracy(model, X, T):\n",
    "    y_true = np.argmax(T, axis=1)\n",
    "    scores = model.predict(X)\n",
    "    y_pred = np.argmax(scores, axis=1)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return acc, y_true, y_pred\n",
    "\n",
    "\n",
    "def print_classification_metrics(y_true, y_pred, label_names=(\"SLI\", \"TD\")):\n",
    "    \"\"\"\n",
    "    accuracy, confusion matrix, per-class precision/recall/F1, macro F1 출력\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    # 전체 accuracy\n",
    "    acc = (y_true == y_pred).mean()\n",
    "\n",
    "    # confusion matrix (2x2)\n",
    "    # 가정: 0 → SLI, 1 → TD\n",
    "    tp_1 = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn_1 = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp_1 = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn_1 = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    print(\"\\nConfusion Matrix (rows = true, cols = pred):\")\n",
    "    print(\"          pred_SLI   pred_TD\")\n",
    "    print(f\"true_SLI   {tn_1:7d}   {fp_1:7d}\")\n",
    "    print(f\"true_TD    {fn_1:7d}   {tp_1:7d}\")\n",
    "\n",
    "    # per-class metrics\n",
    "    per_class = []\n",
    "    for i, label in enumerate(label_names):\n",
    "        tp = np.sum((y_true == i) & (y_pred == i))\n",
    "        fp = np.sum((y_true != i) & (y_pred == i))\n",
    "        fn = np.sum((y_true == i) & (y_pred != i))\n",
    "        support = np.sum(y_true == i)\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1        = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        per_class.append((label, precision, recall, f1, support))\n",
    "\n",
    "    macro_precision = np.mean([x[1] for x in per_class])\n",
    "    macro_recall    = np.mean([x[2] for x in per_class])\n",
    "    macro_f1        = np.mean([x[3] for x in per_class])\n",
    "\n",
    "    print(\"\\nPer-class metrics:\")\n",
    "    print(\"Class   Precision   Recall   F1-score   Support\")\n",
    "    for label, p, r, f1, sup in per_class:\n",
    "        print(f\"{label:5s}  {p:9.4f}  {r:7.4f}  {f1:9.4f}   {sup:7d}\")\n",
    "\n",
    "    print(\"\\nMacro-averaged:\")\n",
    "    print(f\"Precision: {macro_precision:.4f}, Recall: {macro_recall:.4f}, F1: {macro_f1:.4f}\")\n",
    "    print(f\"\\nOverall Accuracy: {acc:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. 메인: 학습 루프\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    # 작업 디렉토리 기준 (예: /DL_project2/gillam 에서 실행)\n",
    "    base_dir = Path(\".\").resolve()\n",
    "\n",
    "    train_csv = base_dir / \"gillam_train.csv\"\n",
    "    dev_csv   = base_dir / \"gillam_dev.csv\"\n",
    "    test_csv  = base_dir / \"gillam_test.csv\"\n",
    "\n",
    "    # 1) train/dev/test 텍스트 & 라벨 로딩 (CHA → clean_text)\n",
    "    print(\"Loading train/dev/test splits ...\")\n",
    "    train_texts, train_labels = load_split_csv(train_csv, base_dir, speakers=(\"CHI\",))\n",
    "    dev_texts, dev_labels     = load_split_csv(dev_csv, base_dir, speakers=(\"CHI\",))\n",
    "    test_texts, test_labels   = load_split_csv(test_csv, base_dir, speakers=(\"CHI\",))\n",
    "\n",
    "    print(f\"Train subjects: {len(train_texts)}, Dev: {len(dev_texts)}, Test: {len(test_texts)}\")\n",
    "\n",
    "    # 2) vocab 은 train 기준으로 생성\n",
    "    vocab, word_to_id = build_vocab(train_texts, min_freq=1, max_size=None)\n",
    "    print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "    # 3) BoW 행렬로 변환\n",
    "    X_train, T_train = build_bow_matrix(train_texts, train_labels, word_to_id)\n",
    "    X_dev, T_dev     = build_bow_matrix(dev_texts, dev_labels, word_to_id)\n",
    "    X_test, T_test   = build_bow_matrix(test_texts, test_labels, word_to_id)\n",
    "\n",
    "    input_size = X_train.shape[1]   # vocab size\n",
    "    hidden_size = 100               # 적당히 (나중에 튜닝)\n",
    "    output_size = 2                 # SLI / TD\n",
    "\n",
    "    print(f\"Input size: {input_size}, Hidden size: {hidden_size}, Output size: {output_size}\")\n",
    "\n",
    "    # 4) 모델 & 옵티마이저 생성\n",
    "    model = TwoLayerNet(input_size=input_size,\n",
    "                        hidden_size=hidden_size,\n",
    "                        output_size=output_size)\n",
    "    optimizer = SGD(lr=0.1)\n",
    "\n",
    "    # 5) 학습 하이퍼파라미터\n",
    "    max_epoch = 30\n",
    "    batch_size = 8\n",
    "\n",
    "    data_size = X_train.shape[0]\n",
    "    max_iters = max(1, data_size // batch_size)\n",
    "\n",
    "    # 6) 학습 루프\n",
    "    for epoch in range(max_epoch):\n",
    "        # shuffle\n",
    "        idx = np.random.permutation(data_size)\n",
    "        X_train = X_train[idx]\n",
    "        T_train = T_train[idx]\n",
    "\n",
    "        total_loss, loss_cnt = 0.0, 0\n",
    "\n",
    "        for it in range(max_iters):\n",
    "            batch_x = X_train[it * batch_size:(it + 1) * batch_size]\n",
    "            batch_t = T_train[it * batch_size:(it + 1) * batch_size]\n",
    "\n",
    "            loss = model.forward(batch_x, batch_t)\n",
    "            model.backward()\n",
    "            optimizer.update(model.params, model.grads)\n",
    "\n",
    "            total_loss += loss\n",
    "            loss_cnt += 1\n",
    "\n",
    "        avg_loss = total_loss / max(1, loss_cnt)\n",
    "\n",
    "        # 매 epoch 끝날 때 dev accuracy 찍기\n",
    "        dev_acc, _, _ = evaluate_accuracy(model, X_dev, T_dev)\n",
    "        print(f\"[Epoch {epoch+1:02d}] loss={avg_loss:.4f}, dev_acc={dev_acc:.4f}\")\n",
    "\n",
    "    # 7) 최종 test 평가 + metrics 출력\n",
    "    test_acc, y_true, y_pred = evaluate_accuracy(model, X_test, T_test)\n",
    "    print(f\"\\n[TEST] accuracy = {test_acc:.4f}\")\n",
    "    print_classification_metrics(y_true, y_pred, label_names=(\"SLI\", \"TD\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aac1a8-53d0-4fe4-9e18-79a9c78b18cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
