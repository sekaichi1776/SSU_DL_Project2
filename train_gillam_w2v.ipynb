{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48624141-d6c1-47b5-a936-5e84049ba793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/dev/test splits ...\n",
      "Train subjects: 540, Dev: 68, Test: 68\n",
      "Vocab size: 3700\n",
      "Building corpus for Word2Vec...\n",
      "Corpus length: 203605, Vocab size: 3700\n",
      "Training pairs: 203603\n",
      "[Word2Vec][Epoch 01] loss=8.2143\n",
      "[Word2Vec][Epoch 02] loss=8.1877\n",
      "[Word2Vec][Epoch 03] loss=7.7430\n",
      "[Word2Vec][Epoch 04] loss=7.0919\n",
      "[Word2Vec][Epoch 05] loss=6.6839\n",
      "Word2Vec training done.\n",
      "Classifier input size: 50, hidden size: 100\n",
      "[Classifier][Epoch 01] loss=0.6236, dev_acc=0.7353\n",
      "[Classifier][Epoch 02] loss=0.6269, dev_acc=0.7353\n",
      "[Classifier][Epoch 03] loss=0.6452, dev_acc=0.7353\n",
      "[Classifier][Epoch 04] loss=0.6128, dev_acc=0.7353\n",
      "[Classifier][Epoch 05] loss=0.6413, dev_acc=0.7353\n",
      "[Classifier][Epoch 06] loss=0.6133, dev_acc=0.7353\n",
      "[Classifier][Epoch 07] loss=0.6496, dev_acc=0.7353\n",
      "[Classifier][Epoch 08] loss=0.6476, dev_acc=0.7353\n",
      "[Classifier][Epoch 09] loss=0.6143, dev_acc=0.2647\n",
      "[Classifier][Epoch 10] loss=0.6226, dev_acc=0.7353\n",
      "[Classifier][Epoch 11] loss=0.6236, dev_acc=0.7353\n",
      "[Classifier][Epoch 12] loss=0.6248, dev_acc=0.7353\n",
      "[Classifier][Epoch 13] loss=0.6227, dev_acc=0.2647\n",
      "[Classifier][Epoch 14] loss=0.6493, dev_acc=0.7353\n",
      "[Classifier][Epoch 15] loss=0.6191, dev_acc=0.7353\n",
      "[Classifier][Epoch 16] loss=0.6221, dev_acc=0.7353\n",
      "[Classifier][Epoch 17] loss=0.6163, dev_acc=0.7353\n",
      "[Classifier][Epoch 18] loss=0.6479, dev_acc=0.7353\n",
      "[Classifier][Epoch 19] loss=0.5957, dev_acc=0.7353\n",
      "[Classifier][Epoch 20] loss=0.6478, dev_acc=0.7353\n",
      "[Classifier][Epoch 21] loss=0.6198, dev_acc=0.7353\n",
      "[Classifier][Epoch 22] loss=0.6112, dev_acc=0.7353\n",
      "[Classifier][Epoch 23] loss=0.6029, dev_acc=0.7353\n",
      "[Classifier][Epoch 24] loss=0.6128, dev_acc=0.7353\n",
      "[Classifier][Epoch 25] loss=0.6126, dev_acc=0.7353\n",
      "[Classifier][Epoch 26] loss=0.6026, dev_acc=0.7353\n",
      "[Classifier][Epoch 27] loss=0.6121, dev_acc=0.7353\n",
      "[Classifier][Epoch 28] loss=0.6158, dev_acc=0.7353\n",
      "[Classifier][Epoch 29] loss=0.6275, dev_acc=0.7353\n",
      "[Classifier][Epoch 30] loss=0.6067, dev_acc=0.7353\n",
      "\n",
      "[TEST] accuracy = 0.7206\n",
      "\n",
      "Confusion Matrix (rows = true, cols = pred):\n",
      "          pred_SLI   pred_TD\n",
      "true_SLI         0        19\n",
      "true_TD          0        49\n",
      "\n",
      "Per-class metrics:\n",
      "Class   Precision   Recall   F1-score   Support\n",
      "SLI       0.0000   0.0000     0.0000        19\n",
      "TD        0.7206   1.0000     0.8376        49\n",
      "\n",
      "Macro-averaged:\n",
      "Precision: 0.3603, Recall: 0.5000, F1: 0.4188\n",
      "\n",
      "Overall Accuracy: 0.7206\n"
     ]
    }
   ],
   "source": [
    "# train_gillam_w2v.py\n",
    "# B) Word2Vec (Simple CBOW, no one-hot) + TwoLayerNet classifier\n",
    "# - Word2Vec: 정수 ID + Embedding, full softmax\n",
    "# - Classifier: TwoLayerNet (from scratch)\n",
    "# - Feature: 각 아이(CHI 발화 전체)의 단어 임베딩 평균\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import extract_utterances  # clean_text 포함된 Utterance 제공\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0. 공통 유틸 (softmax, cross-entropy 등)\n",
    "# ============================================================\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        y = np.exp(x)\n",
    "        y /= y.sum(axis=1, keepdims=True)\n",
    "    else:\n",
    "        x = x - np.max(x)\n",
    "        y = np.exp(x) / np.sum(np.exp(x))\n",
    "    return y\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    \"\"\"y: softmax 출력, t: one-hot 또는 정수 라벨\"\"\"\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(1, -1)\n",
    "        t = t.reshape(1, -1)\n",
    "\n",
    "    # one-hot이면 정수 라벨로 변환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. 레이어들 (Embedding, MatMul, Affine, Sigmoid, SoftmaxWithLoss)\n",
    "# ============================================================\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        # W: (V, D)\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # idx: (N,)\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]  # (N, D)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # dout: (N, D)\n",
    "        W, = self.params\n",
    "        dW = self.grads[0]\n",
    "        dW[...] = 0  # 초기화\n",
    "        np.add.at(dW, self.idx, dout)  # 같은 단어 여러 번 등장할 수 있으므로 add.at 사용\n",
    "        return None\n",
    "\n",
    "\n",
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        self.x = x\n",
    "        out = x.dot(W)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = dout.dot(W.T)\n",
    "        dW = self.x.T.dot(dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        self.x = x\n",
    "        out = x.dot(W) + b\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = dout.dot(W.T)\n",
    "        dW = self.x.T.dot(dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        # one-hot 인 경우\n",
    "        if self.t.size == self.y.size:\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx /= batch_size\n",
    "\n",
    "        return dx * dout\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Optimizer (SGD)\n",
    "# ============================================================\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for p, g in zip(params, grads):\n",
    "            p -= self.lr * g\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. TwoLayerNet (Classifier)\n",
    "# ============================================================\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \"\"\"\n",
    "    입력층 - 은닉층 - 출력층 2층 신경망 (classifier)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "\n",
    "        W1 = 0.01 * np.random.randn(I, H).astype(np.float32)\n",
    "        b1 = np.zeros(H, dtype=np.float32)\n",
    "        W2 = 0.01 * np.random.randn(H, O).astype(np.float32)\n",
    "        b2 = np.zeros(O, dtype=np.float32)\n",
    "\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        score = self.predict(x)\n",
    "        loss = self.loss_layer.forward(score, t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Simple CBOW (Word2Vec, no one-hot)\n",
    "# ============================================================\n",
    "\n",
    "class SimpleCBOW:\n",
    "    \"\"\"\n",
    "    window_size = 1 가정 (양 옆 1개씩, context 2개)\n",
    "    contexts: (N, 2) 정수 ID\n",
    "    target  : (N,)   정수 ID\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype(np.float32)\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype(np.float32)\n",
    "\n",
    "        self.in_layer0 = Embedding(W_in)\n",
    "        self.in_layer1 = Embedding(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        self.word_vecs = W_in  # 입력 가중치가 곧 단어 임베딩\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        # contexts: (N, 2) int\n",
    "        # target  : (N,)   int\n",
    "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
    "        h = 0.5 * (h0 + h1)  # (N, H)\n",
    "        score = self.out_layer.forward(h)  # (N, V)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)      # (N, V)\n",
    "        dh = self.out_layer.backward(ds)         # (N, H)\n",
    "        dh *= 0.5\n",
    "        self.in_layer1.backward(dh)              # in_layer backward는 dW만 누적, dx는 사용 안 함\n",
    "        self.in_layer0.backward(dh)\n",
    "        return None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Word2Vec 데이터 유틸\n",
    "# ============================================================\n",
    "\n",
    "def build_vocab(texts, min_freq=1, max_size=None):\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(t.lower().split())\n",
    "\n",
    "    vocab = [\"<unk>\"]\n",
    "    for w, c in counter.most_common():\n",
    "        if c < min_freq:\n",
    "            continue\n",
    "        vocab.append(w)\n",
    "        if max_size is not None and len(vocab) >= max_size:\n",
    "            break\n",
    "\n",
    "    word_to_id = {w: i for i, w in enumerate(vocab)}\n",
    "    return vocab, word_to_id\n",
    "\n",
    "\n",
    "def build_corpus(texts, word_to_id):\n",
    "    \"\"\"\n",
    "    여러 텍스트를 하나의 corpus로 이어붙인 단어 ID 시퀀스 (1D ndarray) 생성\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    unk_id = word_to_id[\"<unk>\"]\n",
    "    for t in texts:\n",
    "        for w in t.lower().split():\n",
    "            corpus.append(word_to_id.get(w, unk_id))\n",
    "    return np.array(corpus, dtype=np.int64)\n",
    "\n",
    "\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    \"\"\"\n",
    "    window_size=1 기준 CBOW용 contexts, target 생성\n",
    "    contexts: (N, 2) 정수 ID\n",
    "    target  : (N,)   정수 ID\n",
    "    \"\"\"\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    N = len(corpus)\n",
    "    for idx in range(window_size, N - window_size):\n",
    "        target = corpus[idx]\n",
    "        left = corpus[idx - window_size]\n",
    "        right = corpus[idx + window_size]\n",
    "        contexts.append([left, right])\n",
    "        targets.append(target)\n",
    "    return np.array(contexts, dtype=np.int64), np.array(targets, dtype=np.int64)\n",
    "\n",
    "\n",
    "def train_word2vec(texts, vocab, word_to_id,\n",
    "                   embedding_dim=50,\n",
    "                   window_size=1,\n",
    "                   lr=0.1,\n",
    "                   batch_size=64,\n",
    "                   max_epoch=10):\n",
    "    \"\"\"\n",
    "    train_texts(=주로 train split)로 Word2Vec(Simple CBOW) 학습\n",
    "    one-hot을 전혀 만들지 않으므로 메모리 사용량이 훨씬 적음.\n",
    "    \"\"\"\n",
    "    print(\"Building corpus for Word2Vec...\")\n",
    "    corpus = build_corpus(texts, word_to_id)\n",
    "    vocab_size = len(vocab)\n",
    "    print(f\"Corpus length: {len(corpus)}, Vocab size: {vocab_size}\")\n",
    "\n",
    "    contexts_ids, target_ids = create_contexts_target(corpus, window_size=window_size)\n",
    "    print(f\"Training pairs: {len(target_ids)}\")\n",
    "\n",
    "    model = SimpleCBOW(vocab_size, embedding_dim)\n",
    "    optimizer = SGD(lr=lr)\n",
    "\n",
    "    data_size = target_ids.shape[0]\n",
    "    max_iters = max(1, data_size // batch_size)\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "        idx = np.random.permutation(data_size)\n",
    "        contexts_ids = contexts_ids[idx]\n",
    "        target_ids_shuf = target_ids[idx]\n",
    "\n",
    "        total_loss, loss_cnt = 0.0, 0\n",
    "\n",
    "        for it in range(max_iters):\n",
    "            batch_ctx = contexts_ids[it * batch_size:(it + 1) * batch_size]\n",
    "            batch_tgt = target_ids_shuf[it * batch_size:(it + 1) * batch_size]\n",
    "\n",
    "            loss = model.forward(batch_ctx, batch_tgt)\n",
    "            model.backward()\n",
    "            optimizer.update(model.params, model.grads)\n",
    "\n",
    "            total_loss += loss\n",
    "            loss_cnt += 1\n",
    "\n",
    "        avg_loss = total_loss / max(1, loss_cnt)\n",
    "        print(f\"[Word2Vec][Epoch {epoch+1:02d}] loss={avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Word2Vec training done.\")\n",
    "    return model.word_vecs  # (V, embedding_dim)\n",
    "\n",
    "\n",
    "def text_to_w2v_vec(text, word_to_id, word_vecs):\n",
    "    \"\"\"\n",
    "    한 텍스트를 단어 임베딩 평균으로 변환\n",
    "    \"\"\"\n",
    "    tokens = text.lower().split()\n",
    "    ids = []\n",
    "    unk_id = word_to_id[\"<unk>\"]\n",
    "    for w in tokens:\n",
    "        ids.append(word_to_id.get(w, unk_id))\n",
    "\n",
    "    if len(ids) == 0:\n",
    "        return np.zeros(word_vecs.shape[1], dtype=np.float32)\n",
    "    ids = np.array(ids, dtype=np.int64)\n",
    "    vecs = word_vecs[ids]  # (L, D)\n",
    "    return vecs.mean(axis=0)\n",
    "\n",
    "\n",
    "def build_w2v_matrix(texts, labels, word_to_id, word_vecs):\n",
    "    X = np.stack([text_to_w2v_vec(t, word_to_id, word_vecs) for t in texts])  # (N, D)\n",
    "    T = np.eye(2, dtype=np.float32)[labels]  # one-hot (SLI=0, TD=1)\n",
    "    return X, T\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. Gillam 데이터 로더 (CHA → 텍스트)\n",
    "# ============================================================\n",
    "\n",
    "def load_child_text(cha_path: Path, speakers=('CHI',)) -> str:\n",
    "    utts = extract_utterances(str(cha_path), list(speakers))\n",
    "    texts = [u.clean_text for u in utts if u.clean_text]\n",
    "    return \" \".join(texts)\n",
    "\n",
    "\n",
    "def load_split_csv(csv_path: Path, base_dir: Path, speakers=('CHI',)):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    label_to_idx = {\"SLI\": 0, \"TD\": 1}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        cha_rel = row[\"filename\"]          # 예: 'gillam/SLI/5m/xxx.cha'\n",
    "        cha_path = (base_dir / cha_rel).resolve()\n",
    "\n",
    "        text = load_child_text(cha_path, speakers=speakers)\n",
    "\n",
    "        if len(text.strip()) == 0:\n",
    "            print(f\"[WARN] Empty utterance: {cha_path}\")\n",
    "            continue\n",
    "\n",
    "        texts.append(text)\n",
    "        labels.append(label_to_idx[row[\"group\"]])\n",
    "\n",
    "    labels = np.array(labels, dtype=np.int64)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. 평가\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_accuracy(model, X, T):\n",
    "    y_true = np.argmax(T, axis=1)\n",
    "    scores = model.predict(X)\n",
    "    y_pred = np.argmax(scores, axis=1)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return acc, y_true, y_pred\n",
    "\n",
    "\n",
    "def print_classification_metrics(y_true, y_pred, label_names=(\"SLI\", \"TD\")):\n",
    "    \"\"\"\n",
    "    accuracy, confusion matrix, per-class precision/recall/F1, macro F1 출력\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    # 전체 accuracy\n",
    "    acc = (y_true == y_pred).mean()\n",
    "\n",
    "    # confusion matrix (2x2)\n",
    "    # 가정: 0 → SLI, 1 → TD\n",
    "    tp_1 = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn_1 = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp_1 = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn_1 = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    print(\"\\nConfusion Matrix (rows = true, cols = pred):\")\n",
    "    print(\"          pred_SLI   pred_TD\")\n",
    "    print(f\"true_SLI   {tn_1:7d}   {fp_1:7d}\")\n",
    "    print(f\"true_TD    {fn_1:7d}   {tp_1:7d}\")\n",
    "\n",
    "    # per-class metrics\n",
    "    per_class = []\n",
    "    for i, label in enumerate(label_names):\n",
    "        tp = np.sum((y_true == i) & (y_pred == i))\n",
    "        fp = np.sum((y_true != i) & (y_pred == i))\n",
    "        fn = np.sum((y_true == i) & (y_pred != i))\n",
    "        support = np.sum(y_true == i)\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1        = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        per_class.append((label, precision, recall, f1, support))\n",
    "\n",
    "    macro_precision = np.mean([x[1] for x in per_class])\n",
    "    macro_recall    = np.mean([x[2] for x in per_class])\n",
    "    macro_f1        = np.mean([x[3] for x in per_class])\n",
    "\n",
    "    print(\"\\nPer-class metrics:\")\n",
    "    print(\"Class   Precision   Recall   F1-score   Support\")\n",
    "    for label, p, r, f1, sup in per_class:\n",
    "        print(f\"{label:5s}  {p:9.4f}  {r:7.4f}  {f1:9.4f}   {sup:7d}\")\n",
    "\n",
    "    print(\"\\nMacro-averaged:\")\n",
    "    print(f\"Precision: {macro_precision:.4f}, Recall: {macro_recall:.4f}, F1: {macro_f1:.4f}\")\n",
    "    print(f\"\\nOverall Accuracy: {acc:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. 메인\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    base_dir = Path(\".\").resolve()\n",
    "\n",
    "    train_csv = base_dir / \"gillam_train.csv\"\n",
    "    dev_csv   = base_dir / \"gillam_dev.csv\"\n",
    "    test_csv  = base_dir / \"gillam_test.csv\"\n",
    "\n",
    "    # 1) train/dev/test 텍스트 & 라벨 로딩\n",
    "    print(\"Loading train/dev/test splits ...\")\n",
    "    train_texts, train_labels = load_split_csv(train_csv, base_dir, speakers=(\"CHI\",))\n",
    "    dev_texts, dev_labels     = load_split_csv(dev_csv, base_dir, speakers=(\"CHI\",))\n",
    "    test_texts, test_labels   = load_split_csv(test_csv, base_dir, speakers=(\"CHI\",))\n",
    "\n",
    "    print(f\"Train subjects: {len(train_texts)}, Dev: {len(dev_texts)}, Test: {len(test_texts)}\")\n",
    "\n",
    "    # 2) train 텍스트 기준 vocab 생성\n",
    "    vocab, word_to_id = build_vocab(train_texts, min_freq=1, max_size=None)\n",
    "    print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "    # 3) Word2Vec(Simple CBOW) 학습 (train 텍스트만 사용)\n",
    "    embedding_dim = 50\n",
    "    w2v_lr = 0.05          # 살짝 낮춰도 됨\n",
    "    w2v_batch_size = 128   # 배치 크기 키워도 메모리 OK\n",
    "    w2v_epochs = 5         # 처음엔 5 정도로 짧게\n",
    "\n",
    "    word_vecs = train_word2vec(\n",
    "        train_texts,\n",
    "        vocab,\n",
    "        word_to_id,\n",
    "        embedding_dim=embedding_dim,\n",
    "        window_size=1,\n",
    "        lr=w2v_lr,\n",
    "        batch_size=w2v_batch_size,\n",
    "        max_epoch=w2v_epochs\n",
    "    )\n",
    "\n",
    "    # 4) 각 subject 텍스트를 임베딩 평균 벡터로 변환\n",
    "    X_train, T_train = build_w2v_matrix(train_texts, train_labels, word_to_id, word_vecs)\n",
    "    X_dev, T_dev     = build_w2v_matrix(dev_texts, dev_labels, word_to_id, word_vecs)\n",
    "    X_test, T_test   = build_w2v_matrix(test_texts, test_labels, word_to_id, word_vecs)\n",
    "\n",
    "    input_size = embedding_dim         # Word2Vec 차원\n",
    "    hidden_size = 100                  # classifier hidden size\n",
    "    output_size = 2                    # SLI / TD\n",
    "\n",
    "    print(f\"Classifier input size: {input_size}, hidden size: {hidden_size}\")\n",
    "\n",
    "    model = TwoLayerNet(input_size=input_size,\n",
    "                        hidden_size=hidden_size,\n",
    "                        output_size=output_size)\n",
    "    optimizer = SGD(lr=0.1)\n",
    "\n",
    "    max_epoch = 30\n",
    "    batch_size = 8\n",
    "    data_size = X_train.shape[0]\n",
    "    max_iters = max(1, data_size // batch_size)\n",
    "\n",
    "    # 5) classifier 학습 루프\n",
    "    for epoch in range(max_epoch):\n",
    "        idx = np.random.permutation(data_size)\n",
    "        X_train = X_train[idx]\n",
    "        T_train = T_train[idx]\n",
    "\n",
    "        total_loss, loss_cnt = 0.0, 0\n",
    "\n",
    "        for it in range(max_iters):\n",
    "            batch_x = X_train[it * batch_size:(it + 1) * batch_size]\n",
    "            batch_t = T_train[it * batch_size:(it + 1) * batch_size]\n",
    "\n",
    "            loss = model.forward(batch_x, batch_t)\n",
    "            model.backward()\n",
    "            optimizer.update(model.params, model.grads)\n",
    "\n",
    "            total_loss += loss\n",
    "            loss_cnt += 1\n",
    "\n",
    "        avg_loss = total_loss / max(1, loss_cnt)\n",
    "\n",
    "        dev_acc, _, _ = evaluate_accuracy(model, X_dev, T_dev)\n",
    "        print(f\"[Classifier][Epoch {epoch+1:02d}] loss={avg_loss:.4f}, dev_acc={dev_acc:.4f}\")\n",
    "\n",
    "    # 6) 최종 test 평가 + metrics 출력\n",
    "    test_acc, y_true, y_pred = evaluate_accuracy(model, X_test, T_test)\n",
    "    print(f\"\\n[TEST] accuracy = {test_acc:.4f}\")\n",
    "    print_classification_metrics(y_true, y_pred, label_names=(\"SLI\", \"TD\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0386d3-1e86-45db-bd17-324c4519aea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
