{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8f5eff-ee4c-492c-bda4-f4402f46dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/dev/test splits ...\n",
      "Train subjects: 540, Dev: 68, Test: 68\n",
      "Vocab size: 3701\n",
      "RNN config: vocab=3701, emb=50, hidden=100, max_len=100\n",
      "[RNN][Epoch 01] loss=0.6628, dev_acc=0.7353\n",
      "[RNN][Epoch 02] loss=0.6223, dev_acc=0.7353\n",
      "[RNN][Epoch 03] loss=0.6027, dev_acc=0.7353\n",
      "[RNN][Epoch 04] loss=0.5891, dev_acc=0.7353\n",
      "[RNN][Epoch 05] loss=0.5826, dev_acc=0.7353\n",
      "[RNN][Epoch 06] loss=0.5824, dev_acc=0.7353\n",
      "[RNN][Epoch 07] loss=0.5790, dev_acc=0.7353\n",
      "[RNN][Epoch 08] loss=0.5763, dev_acc=0.7353\n",
      "[RNN][Epoch 09] loss=0.5773, dev_acc=0.7353\n",
      "[RNN][Epoch 10] loss=0.5769, dev_acc=0.7353\n",
      "[RNN][Epoch 11] loss=0.5785, dev_acc=0.7353\n",
      "[RNN][Epoch 12] loss=0.5766, dev_acc=0.7353\n",
      "[RNN][Epoch 13] loss=0.5766, dev_acc=0.7353\n",
      "[RNN][Epoch 14] loss=0.5784, dev_acc=0.7353\n",
      "[RNN][Epoch 15] loss=0.5746, dev_acc=0.7353\n",
      "[RNN][Epoch 16] loss=0.5765, dev_acc=0.7353\n",
      "[RNN][Epoch 17] loss=0.5765, dev_acc=0.7353\n",
      "[RNN][Epoch 18] loss=0.5784, dev_acc=0.7353\n",
      "[RNN][Epoch 19] loss=0.5783, dev_acc=0.7353\n",
      "[RNN][Epoch 20] loss=0.5745, dev_acc=0.7353\n",
      "[RNN][Epoch 21] loss=0.5764, dev_acc=0.7353\n",
      "[RNN][Epoch 22] loss=0.5747, dev_acc=0.7353\n",
      "[RNN][Epoch 23] loss=0.5765, dev_acc=0.7353\n",
      "[RNN][Epoch 24] loss=0.5746, dev_acc=0.7353\n",
      "[RNN][Epoch 25] loss=0.5746, dev_acc=0.7353\n",
      "[RNN][Epoch 26] loss=0.5745, dev_acc=0.7353\n",
      "[RNN][Epoch 27] loss=0.5746, dev_acc=0.7353\n",
      "[RNN][Epoch 28] loss=0.5784, dev_acc=0.7353\n",
      "[RNN][Epoch 29] loss=0.5784, dev_acc=0.7353\n",
      "[RNN][Epoch 30] loss=0.5784, dev_acc=0.7353\n",
      "\n",
      "[TEST] accuracy = 0.7206, best_dev = 0.7353\n",
      "\n",
      "Confusion Matrix (rows = true, cols = pred):\n",
      "          pred_SLI   pred_TD\n",
      "true_SLI         0        19\n",
      "true_TD          0        49\n",
      "\n",
      "Per-class metrics:\n",
      "Class   Precision   Recall   F1-score   Support\n",
      "SLI       0.0000   0.0000     0.0000        19\n",
      "TD        0.7206   1.0000     0.8376        49\n",
      "\n",
      "Macro-averaged:\n",
      "Precision: 0.3603, Recall: 0.5000, F1: 0.4188\n",
      "\n",
      "Overall Accuracy: 0.7206\n"
     ]
    }
   ],
   "source": [
    "# train_gillam_rnn.py\n",
    "# C) SimpleRNN 기반 SLI/TD 분류기 (프레임워크 없이, 단일 파일)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import extract_utterances  # clean_text 포함된 Utterance 제공\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0. 공통 유틸 (softmax, cross-entropy 등)\n",
    "# ============================================================\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        y = np.exp(x)\n",
    "        y /= y.sum(axis=1, keepdims=True)\n",
    "    else:\n",
    "        x = x - np.max(x)\n",
    "        y = np.exp(x) / np.sum(np.exp(x))\n",
    "    return y\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    \"\"\"y: softmax 출력, t: one-hot 또는 정수 라벨\"\"\"\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(1, -1)\n",
    "        t = t.reshape(1, -1)\n",
    "\n",
    "    # one-hot이면 정수 라벨로 변환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. 레이어들 (Embedding, SimpleRNN, Affine, SoftmaxWithLoss)\n",
    "# ============================================================\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        # W: (V, D)\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # idx: (N, T) int\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]  # (N, T, D)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # dout: (N, T, D)\n",
    "        W, = self.params\n",
    "        dW = self.grads[0]\n",
    "        dW[...] = 0\n",
    "        idx = self.idx\n",
    "        N, T, D = dout.shape\n",
    "        dout2 = dout.reshape(N * T, D)\n",
    "        idx2 = idx.reshape(N * T)\n",
    "        np.add.at(dW, idx2, dout2)\n",
    "        return None\n",
    "\n",
    "\n",
    "class SimpleRNNLayer:\n",
    "    \"\"\"\n",
    "    단일 RNN layer (전체 시퀀스 unroll)\n",
    "    입력: x (N, T, D)\n",
    "    출력: h_last (N, H)\n",
    "    \"\"\"\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        # Wx: (D, H), Wh: (H, H), b: (H,)\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "\n",
    "        self.xs = None      # 리스트 [x_t] (각각 (N, D))\n",
    "        self.hs = None      # 리스트 [h_t] (각각 (N, H))\n",
    "        self.h0 = None      # 초기 hidden (N, H)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = x.shape\n",
    "        H = Wh.shape[0]\n",
    "\n",
    "        self.xs = []\n",
    "        self.hs = []\n",
    "        self.h0 = np.zeros((N, H), dtype=np.float32)\n",
    "\n",
    "        h_prev = self.h0\n",
    "        for t in range(T):\n",
    "            xt = x[:, t, :]  # (N, D)\n",
    "            a = xt.dot(Wx) + h_prev.dot(Wh) + b  # (N, H)\n",
    "            h = np.tanh(a)\n",
    "            self.xs.append(xt)\n",
    "            self.hs.append(h)\n",
    "            h_prev = h\n",
    "\n",
    "        # 마지막 hidden state만 반환\n",
    "        return self.hs[-1]\n",
    "\n",
    "    def backward(self, dh_last):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, D = self.xs[0].shape\n",
    "        T = len(self.xs)\n",
    "        H = Wh.shape[0]\n",
    "\n",
    "        dWx, dWh, db = self.grads\n",
    "        dWx[...] = 0\n",
    "        dWh[...] = 0\n",
    "        db[...] = 0\n",
    "\n",
    "        dxs = [np.zeros((N, D), dtype=np.float32) for _ in range(T)]\n",
    "\n",
    "        dh_next = dh_last  # (N, H)\n",
    "\n",
    "        for t in reversed(range(T)):\n",
    "            xt = self.xs[t]\n",
    "            ht = self.hs[t]\n",
    "            h_prev = self.h0 if t == 0 else self.hs[t - 1]\n",
    "\n",
    "            # tanh backward\n",
    "            da = dh_next * (1.0 - ht ** 2)  # (N, H)\n",
    "\n",
    "            dWx += xt.T.dot(da)             # (D, H)\n",
    "            dWh += h_prev.T.dot(da)         # (H, H)\n",
    "            db += da.sum(axis=0)            # (H,)\n",
    "\n",
    "            dx = da.dot(Wx.T)               # (N, D)\n",
    "            dh_prev = da.dot(Wh.T)          # (N, H)\n",
    "\n",
    "            dxs[t] = dx\n",
    "            dh_next = dh_prev\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        dxs = np.stack(dxs, axis=1)  # (N, T, D)\n",
    "        return dxs\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        self.x = x\n",
    "        out = x.dot(W) + b\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = dout.dot(W.T)\n",
    "        dW = self.x.T.dot(dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        if self.t.size == self.y.size:\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx /= batch_size\n",
    "\n",
    "        return dx * dout\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Optimizer (SGD)\n",
    "# ============================================================\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for p, g in zip(params, grads):\n",
    "            p -= self.lr * g\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. RNNClassifier\n",
    "# ============================================================\n",
    "\n",
    "class RNNClassifier:\n",
    "    \"\"\"\n",
    "    Embedding → SimpleRNN(last hidden) → Affine → SoftmaxWithLoss\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
    "        V, D, H, O = vocab_size, embedding_dim, hidden_size, output_size\n",
    "\n",
    "        embed_W = 0.01 * np.random.randn(V, D).astype(np.float32)\n",
    "        Wx = 0.01 * np.random.randn(D, H).astype(np.float32)\n",
    "        Wh = 0.01 * np.random.randn(H, H).astype(np.float32)\n",
    "        b_rnn = np.zeros(H, dtype=np.float32)\n",
    "        W_aff = 0.01 * np.random.randn(H, O).astype(np.float32)\n",
    "        b_aff = np.zeros(O, dtype=np.float32)\n",
    "\n",
    "        self.embed = Embedding(embed_W)\n",
    "        self.rnn = SimpleRNNLayer(Wx, Wh, b_rnn)\n",
    "        self.affine = Affine(W_aff, b_aff)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        for layer in [self.embed, self.rnn, self.affine]:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, x_ids):\n",
    "        x = self.embed.forward(x_ids)            # (N, T, D)\n",
    "        h_last = self.rnn.forward(x)            # (N, H)\n",
    "        score = self.affine.forward(h_last)     # (N, O)\n",
    "        return score\n",
    "\n",
    "    def forward(self, x_ids, t):\n",
    "        score = self.predict(x_ids)\n",
    "        loss = self.loss_layer.forward(score, t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)      # (N, O)\n",
    "        dh_last = self.affine.backward(ds)       # (N, H)\n",
    "        dx = self.rnn.backward(dh_last)          # (N, T, D)\n",
    "        self.embed.backward(dx)                  # dW에 누적\n",
    "        return None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Gillam 데이터 로더 (CHA → 텍스트)\n",
    "# ============================================================\n",
    "\n",
    "def load_child_text(cha_path: Path, speakers=('CHI',)) -> str:\n",
    "    utts = extract_utterances(str(cha_path), list(speakers))\n",
    "    texts = [u.clean_text for u in utts if u.clean_text]\n",
    "    return \" \".join(texts)\n",
    "\n",
    "\n",
    "def load_split_csv(csv_path: Path, base_dir: Path, speakers=('CHI',)):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    label_to_idx = {\"SLI\": 0, \"TD\": 1}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        cha_rel = row[\"filename\"]          # 예: 'gillam/SLI/5m/xxx.cha'\n",
    "        cha_path = (base_dir / cha_rel).resolve()\n",
    "\n",
    "        text = load_child_text(cha_path, speakers=speakers)\n",
    "\n",
    "        if len(text.strip()) == 0:\n",
    "            print(f\"[WARN] Empty utterance: {cha_path}\")\n",
    "            continue\n",
    "\n",
    "        texts.append(text)\n",
    "        labels.append(label_to_idx[row[\"group\"]])\n",
    "\n",
    "    labels = np.array(labels, dtype=np.int64)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Vocab + 시퀀스 변환\n",
    "# ============================================================\n",
    "\n",
    "def build_vocab(texts, min_freq=1, max_size=None):\n",
    "    \"\"\"\n",
    "    vocab[0] = <pad>, vocab[1] = <unk>\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(t.lower().split())\n",
    "\n",
    "    vocab = [\"<pad>\", \"<unk>\"]\n",
    "    for w, c in counter.most_common():\n",
    "        if c < min_freq:\n",
    "            continue\n",
    "        vocab.append(w)\n",
    "        if max_size is not None and len(vocab) >= max_size:\n",
    "            break\n",
    "\n",
    "    word_to_id = {w: i for i, w in enumerate(vocab)}\n",
    "    return vocab, word_to_id\n",
    "\n",
    "\n",
    "def text_to_ids(text, word_to_id, max_len):\n",
    "    pad_id = word_to_id[\"<pad>\"]\n",
    "    unk_id = word_to_id[\"<unk>\"]\n",
    "\n",
    "    tokens = text.lower().split()\n",
    "    ids = [word_to_id.get(w, unk_id) for w in tokens]\n",
    "\n",
    "    if len(ids) >= max_len:\n",
    "        ids = ids[:max_len]\n",
    "    else:\n",
    "        ids += [pad_id] * (max_len - len(ids))\n",
    "\n",
    "    return np.array(ids, dtype=np.int64)\n",
    "\n",
    "\n",
    "def build_rnn_input(texts, labels, word_to_id, max_len):\n",
    "    X_ids = np.stack([text_to_ids(t, word_to_id, max_len) for t in texts])  # (N, T)\n",
    "    T = np.eye(2, dtype=np.float32)[labels]  # one-hot\n",
    "    return X_ids, T\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. 평가\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_accuracy(model, X_ids, T):\n",
    "    y_true = np.argmax(T, axis=1)\n",
    "    scores = model.predict(X_ids)\n",
    "    y_pred = np.argmax(scores, axis=1)\n",
    "    acc = (y_true == y_pred).mean()\n",
    "    return acc, y_true, y_pred\n",
    "\n",
    "\n",
    "def print_classification_metrics(y_true, y_pred, label_names=(\"SLI\", \"TD\")):\n",
    "    \"\"\"\n",
    "    accuracy, confusion matrix, per-class precision/recall/F1, macro F1 출력\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    # 전체 accuracy\n",
    "    acc = (y_true == y_pred).mean()\n",
    "\n",
    "    # confusion matrix (2x2) 0=SLI, 1=TD\n",
    "    tp_1 = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn_1 = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp_1 = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn_1 = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    print(\"\\nConfusion Matrix (rows = true, cols = pred):\")\n",
    "    print(\"          pred_SLI   pred_TD\")\n",
    "    print(f\"true_SLI   {tn_1:7d}   {fp_1:7d}\")\n",
    "    print(f\"true_TD    {fn_1:7d}   {tp_1:7d}\")\n",
    "\n",
    "    # per-class metrics\n",
    "    per_class = []\n",
    "    for i, label in enumerate(label_names):\n",
    "        tp = np.sum((y_true == i) & (y_pred == i))\n",
    "        fp = np.sum((y_true != i) & (y_pred == i))\n",
    "        fn = np.sum((y_true == i) & (y_pred != i))\n",
    "        support = np.sum(y_true == i)\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1        = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        per_class.append((label, precision, recall, f1, support))\n",
    "\n",
    "    macro_precision = np.mean([x[1] for x in per_class])\n",
    "    macro_recall    = np.mean([x[2] for x in per_class])\n",
    "    macro_f1        = np.mean([x[3] for x in per_class])\n",
    "\n",
    "    print(\"\\nPer-class metrics:\")\n",
    "    print(\"Class   Precision   Recall   F1-score   Support\")\n",
    "    for label, p, r, f1, sup in per_class:\n",
    "        print(f\"{label:5s}  {p:9.4f}  {r:7.4f}  {f1:9.4f}   {sup:7d}\")\n",
    "\n",
    "    print(\"\\nMacro-averaged:\")\n",
    "    print(f\"Precision: {macro_precision:.4f}, Recall: {macro_recall:.4f}, F1: {macro_f1:.4f}\")\n",
    "    print(f\"\\nOverall Accuracy: {acc:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. 메인\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    base_dir = Path(\".\").resolve()\n",
    "\n",
    "    train_csv = base_dir / \"gillam_train.csv\"\n",
    "    dev_csv   = base_dir / \"gillam_dev.csv\"\n",
    "    test_csv  = base_dir / \"gillam_test.csv\"\n",
    "\n",
    "    # 1) train/dev/test 텍스트 & 라벨 로딩\n",
    "    print(\"Loading train/dev/test splits ...\")\n",
    "    train_texts, train_labels = load_split_csv(train_csv, base_dir, speakers=(\"CHI\",))\n",
    "    dev_texts, dev_labels     = load_split_csv(dev_csv, base_dir, speakers=(\"CHI\",))\n",
    "    test_texts, test_labels   = load_split_csv(test_csv, base_dir, speakers=(\"CHI\",))\n",
    "\n",
    "    print(f\"Train subjects: {len(train_texts)}, Dev: {len(dev_texts)}, Test: {len(test_texts)}\")\n",
    "\n",
    "    # 2) vocab 생성 (train 기준)\n",
    "    vocab, word_to_id = build_vocab(train_texts, min_freq=1, max_size=None)\n",
    "    print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "    # 3) 시퀀스 변환\n",
    "    max_len = 100  # 한 아이당 최대 토큰 수\n",
    "    X_train_ids, T_train = build_rnn_input(train_texts, train_labels, word_to_id, max_len)\n",
    "    X_dev_ids, T_dev     = build_rnn_input(dev_texts, dev_labels, word_to_id, max_len)\n",
    "    X_test_ids, T_test   = build_rnn_input(test_texts, test_labels, word_to_id, max_len)\n",
    "\n",
    "    # 4) 모델 설정\n",
    "    vocab_size   = len(vocab)\n",
    "    embedding_dim = 50\n",
    "    hidden_size   = 100\n",
    "    output_size   = 2\n",
    "\n",
    "    print(f\"RNN config: vocab={vocab_size}, emb={embedding_dim}, hidden={hidden_size}, max_len={max_len}\")\n",
    "\n",
    "    model = RNNClassifier(vocab_size, embedding_dim, hidden_size, output_size)\n",
    "    optimizer = SGD(lr=0.01)\n",
    "\n",
    "    max_epoch = 30\n",
    "    batch_size = 8\n",
    "    data_size = X_train_ids.shape[0]\n",
    "    max_iters = max(1, data_size // batch_size)\n",
    "\n",
    "    # Early stopping용\n",
    "    best_dev = 0.0\n",
    "    best_params = [p.copy() for p in model.params]\n",
    "\n",
    "    # 5) 학습 루프\n",
    "    for epoch in range(max_epoch):\n",
    "        idx = np.random.permutation(data_size)\n",
    "        X_train_ids = X_train_ids[idx]\n",
    "        T_train = T_train[idx]\n",
    "\n",
    "        total_loss, loss_cnt = 0.0, 0\n",
    "\n",
    "        for it in range(max_iters):\n",
    "            batch_x = X_train_ids[it * batch_size:(it + 1) * batch_size]\n",
    "            batch_t = T_train[it * batch_size:(it + 1) * batch_size]\n",
    "\n",
    "            loss = model.forward(batch_x, batch_t)\n",
    "            model.backward()\n",
    "            optimizer.update(model.params, model.grads)\n",
    "\n",
    "            total_loss += loss\n",
    "            loss_cnt += 1\n",
    "\n",
    "        avg_loss = total_loss / max(1, loss_cnt)\n",
    "        dev_acc, _, _ = evaluate_accuracy(model, X_dev_ids, T_dev)\n",
    "        print(f\"[RNN][Epoch {epoch+1:02d}] loss={avg_loss:.4f}, dev_acc={dev_acc:.4f}\")\n",
    "\n",
    "        if dev_acc > best_dev:\n",
    "            best_dev = dev_acc\n",
    "            best_params = [p.copy() for p in model.params]\n",
    "\n",
    "    # best 모델로 롤백\n",
    "    for p, bp in zip(model.params, best_params):\n",
    "        p[...] = bp\n",
    "\n",
    "    # 6) 최종 test 평가 + 상세 metric 출력\n",
    "    test_acc, y_true, y_pred = evaluate_accuracy(model, X_test_ids, T_test)\n",
    "    print(f\"\\n[TEST] accuracy = {test_acc:.4f}, best_dev = {best_dev:.4f}\")\n",
    "    print_classification_metrics(y_true, y_pred, label_names=(\"SLI\", \"TD\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd2fbc0-b054-4971-95c9-10fd61c2065f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
